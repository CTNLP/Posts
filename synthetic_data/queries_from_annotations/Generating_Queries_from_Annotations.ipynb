{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9-J4uIOxFZ_"
   },
   "source": [
    "# Generating Synthetic Training Data for Semantic Parsers\n",
    "\n",
    "Even in the age of pre-trained Large Language Models (LLMs), we still need to train or fine-tune models for natural language tasks, to make them better at specific tasks and/or because we want to use a smaller model that will incur lower costs. One approach that has become popular is to use a very large and capable LLM to take \"naturally occurring\" text and annotate it with whatever output we want to train the smaller model to produce. However, I think that it may often be more effective to do the reverse.\n",
    "\n",
    "I will illustrate what I mean using the task of semantic parsing - mapping natural language to something that can be understood by a machine. In order to produce semantic parsing data, I will suggest we:\n",
    "\n",
    "1. **Generate** a wide variety of machine-executable commands (annotations).\n",
    "2. **Synthesize** natural language questions corresponding to those commands.\n",
    "\n",
    "This idea is heavily inspired by the methodology in the paper \\\"[Building a Semantic Parser Overnight](https://aclanthology.org/P15-1129/)\\\" by Wang, Berant, and Liang. While the original paper used crowdworkers to paraphrase stilted, automatically generated descriptions into natural language, we can now leverage LLMs for this step, allowing for massive scalability.\n",
    "\n",
    "Why do I think this is better? As you will see below, it has the following advantages:\n",
    "\n",
    "- LLMs are built to generate natural language. Language is also inherently less precision-oriented than anything machine readable, which makes this task easier for the LLM.\n",
    "- It is far easier to make sure that you cover all possible cases. Your users may not regularly ask a certain type of question because your current system does not understand it correctly, and your system does not get better because you have no queries to train on. So you are stuck in an endless cycle. But possible gold outputs can generally be enumerated.\n",
    "- It will be easier for a human to verify the resulting data if they can assume the code is correct.\n",
    "\n",
    "## A Toy Example: A Restaurant Booking API\n",
    "\n",
    "Let's demonstrate this with a simple restaurant booking API. We'll imagine our API has the following capabilities:\n",
    "\n",
    "* **`find_restaurant`**: Find restaurants by cuisine, rating, or both\n",
    "* **`sort_restaurants`**: Sort restaurants from best to worst rating\n",
    "* **`book_restaurant`**: Book a table at a specific restaurant for a given time\n",
    "\n",
    "First, we will define some Python commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcX-oCmAxnFI"
   },
   "outputs": [],
   "source": [
    "# let's write down the actual Python API for concreteness\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "restaurants = [\n",
    "    {\n",
    "        \"name\": \"The Pizza Place\",\n",
    "        \"cuisine\": \"Italian\",\n",
    "        \"rating\": 4.5,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Curry House\",\n",
    "        \"cuisine\": \"Indian\",\n",
    "        \"rating\": 4.2,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Taco Town\",\n",
    "        \"cuisine\": \"Mexican\",\n",
    "        \"rating\": 3.8,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Playa Bar\",\n",
    "        \"cuisine\": \"Mexican\",\n",
    "        \"rating\": 4.7,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Spice Merchant\",\n",
    "        \"cuisine\": \"Indian\",\n",
    "        \"rating\": 4.1,\n",
    "    },\n",
    "]\n",
    "\n",
    "class Booking:\n",
    "    def __init__(self, restaurant, time):\n",
    "        self.restaurant = restaurant\n",
    "        self.time = time\n",
    "\n",
    "bookings = []\n",
    "\n",
    "\n",
    "def find_restaurant(cuisine=None, rating_gt=None):\n",
    "    if cuisine is None and rating_gt is None:\n",
    "        return restaurants\n",
    "    else:\n",
    "        results = []\n",
    "        for r in restaurants:\n",
    "            if cuisine and r[\"cuisine\"] != cuisine:\n",
    "                continue\n",
    "            if rating_gt and r[\"rating\"] <= rating_gt:\n",
    "                continue\n",
    "            results.append(r)\n",
    "        return results\n",
    "\n",
    "def make_booking(restaurant, time):\n",
    "    bookings.append(Booking(restaurant, time))\n",
    "\n",
    "def sort_restaurants(restaurants):\n",
    "    return sorted(restaurants, key=lambda r: r[\"rating\"], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsxQn7xEz7rb"
   },
   "source": [
    "It seems simplistic, but this interface already allows for a lot of complexity through **compositionality**, e.g., the equivalent of \"Make a booking for tomorrow at the best-rated Indian restaurant\".\n",
    "\n",
    "A system would need to parse this into a sequence of operations:\n",
    "1.  **Find all Indian restaurants:**\n",
    "    `find_restaurant(cuisine=\"Indian\")`\n",
    "2.  **Sort them by rating to find the best one:**\n",
    "    `sort_restaurants(restaurants=results_from_step_1)`\n",
    "3.  **Take the top result and book it:**\n",
    "    `make_booking(restaurant=best_restaurant_from_step_2, time=\"tomorrow\")`\n",
    "\n",
    "As a single, nested Python call, this composition looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-eGEfkCO0h2K"
   },
   "outputs": [],
   "source": [
    "make_booking(sort_restaurants(find_restaurant(cuisine=\"Indian\"))[0], datetime.now() + timedelta(days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1t9RFj5E8l0"
   },
   "source": [
    "The complexity we've seen justifies building a natural language interface. To do this effectively, we need to train a fast, specialized semantic parser and so we need to create data.\n",
    "\n",
    "This is where our core strategy comes into play. Instead of waiting for user queries to annotate, we will **generate the machine-readable commands first** and then synthesize the corresponding natural language. The structured commands we want to generate follow predictable patterns and can be systematically enumerated, making this generation easy.\n",
    "\n",
    "Let's start by writing a simple generator to produce a variety of valid API calls. For each call, we'll also generate a canonical, \"stilted\" description of what it does. This description will serve as the basis for our later LLM paraphrasing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLNnQe3GF01h",
    "outputId": "9c1e2263-a2c6-4391-90dc-772c70ee468e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('find_restaurant(cuisine=\"Italian\",rating_gt=2)', 'find restaurants that has cuisine Italian with a rating above  at least 2 rating')\n",
      "('find_restaurant(cuisine=\"Indian\",)', 'find restaurants that has cuisine Indian')\n",
      "('find_restaurant()', 'find restaurants')\n",
      "('find_restaurant(cuisine=\"Indian\",rating_gt=2)', 'find restaurants that has cuisine Indian with a rating above  at least 2 rating')\n",
      "('find_restaurant(rating_gt=4)', 'find restaurants with a rating above  at least 4 rating')\n",
      "('make_booking(restaurant=find_restaurant(cuisine=\"Mexican\",), time=datetime.now() + timedelta(days=1))', 'make a booking for tomorrow at find restaurants that has cuisine Mexican')\n",
      "('make_booking(restaurant=find_restaurant(cuisine=\"Italian\",), time=datetime.now() + timedelta(days=1))', 'make a booking for tomorrow at find restaurants that has cuisine Italian')\n",
      "('make_booking(restaurant=find_restaurant(rating_gt=1), time=datetime.now() + timedelta(days=7))', 'make a booking for one week from now at find restaurants with a rating above  at least 1 rating')\n",
      "('make_booking(restaurant=find_restaurant(cuisine=\"Mexican\",rating_gt=3), time=datetime.now() + timedelta(days=1))', 'make a booking for tomorrow at find restaurants that has cuisine Mexican with a rating above  at least 3 rating')\n",
      "('make_booking(restaurant=find_restaurant(rating_gt=4), time=datetime.now() + timedelta(days=7))', 'make a booking for one week from now at find restaurants with a rating above  at least 4 rating')\n",
      "('sort_restaurants(find_restaurant())[:5]', 'the best 5 find restaurants')\n",
      "('sort_restaurants(find_restaurant(cuisine=\"Italian\",rating_gt=3))[:1]', 'the best 1 find restaurants that has cuisine Italian with a rating above  at least 3 rating')\n",
      "('sort_restaurants(find_restaurant(rating_gt=2))[:4]', 'the best 4 find restaurants with a rating above  at least 2 rating')\n",
      "('sort_restaurants(find_restaurant(rating_gt=4))[:4]', 'the best 4 find restaurants with a rating above  at least 4 rating')\n",
      "('sort_restaurants(find_restaurant())[:1]', 'the best 1 find restaurants')\n"
     ]
    }
   ],
   "source": [
    "# we use random number generation to get variance in what we generate\n",
    "import random\n",
    "random.seed(235711)\n",
    "\n",
    "cuisines = [\n",
    "    \"Italian\",\n",
    "    \"Indian\",\n",
    "    \"Mexican\"\n",
    "] # could be a random draw from all cuisines for all restaurants in the database\n",
    "\n",
    "# we will produce two strings, one for the instructions, one for the \"natural language\" description\n",
    "def make_find_restaurant_expression() -> tuple[str,str]:\n",
    "  code = \"find_restaurant(\"\n",
    "  description = \"find restaurants\"\n",
    "\n",
    "  if random.uniform(0,1) > 0.5: # we also want some simple expressions\n",
    "    cuisine_code, cuisine_text = make_cuisine_expression()\n",
    "    code += f\"cuisine={cuisine_code},\"\n",
    "    description += f\" that has cuisine {cuisine_text}\"\n",
    "  if random.uniform(0,1) > 0.5: # we also want some simple expressions\n",
    "    rating_gt_code, rating_gt_text = make_rating_gt_expression()\n",
    "    code += f\"rating_gt={rating_gt_code}\"\n",
    "    description += f\" with a rating above {rating_gt_text}\"\n",
    "  code += \")\"\n",
    "  return code, description\n",
    "\n",
    "def make_cuisine_expression() -> tuple[str,str]:\n",
    "  cuisine = random.choice(cuisines)\n",
    "  return f\"\\\"{cuisine}\\\"\", f\"{cuisine}\"\n",
    "\n",
    "def make_rating_gt_expression() -> tuple[str,str]:\n",
    "  # it may be a bit weird for someone to ask for a rating of 1 or better, but we can just ensure coverage\n",
    "  rating = random.choice(range(1,5))\n",
    "  return f\"{rating}\", f\" at least {rating} rating\"\n",
    "\n",
    "def make_sort_resturants_expression() -> tuple[str,str]:\n",
    "  # say we support up to 10 choices\n",
    "  num_to_pick = random.choice(range(1,10))\n",
    "  restaurants_expression, restaurants_description = make_find_restaurant_expression()\n",
    "  code = f\"sort_restaurants({restaurants_expression})[:{num_to_pick}]\"\n",
    "  description = f\"the best {num_to_pick} {restaurants_description}\"\n",
    "  return code, description\n",
    "\n",
    "def make_make_booking_expression() -> tuple[str,str]:\n",
    "  restaurant_expression, restaurant_description = make_find_restaurant_expression()\n",
    "  time_expression, time_description = make_time_expression()\n",
    "  code = f\"make_booking(restaurant={restaurant_expression}, time={time_expression})\"\n",
    "  description = f\"make a booking for {time_description} at {restaurant_description}\"\n",
    "  return code, description\n",
    "\n",
    "# we will be lazy for time and only cover today, tomorrow and one week from now\n",
    "time_exp =[\n",
    "    (\"datetime.now()\", \"today\"),\n",
    "    (\"datetime.now() + timedelta(days=1)\", \"tomorrow\"),\n",
    "    (\"datetime.now() + timedelta(days=7)\", \"one week from now\")\n",
    "]\n",
    "\n",
    "def make_time_expression() -> tuple[str,str]:\n",
    "  # TODO :) : replace with logic to pick a time and then prompt an LLM to describe it\n",
    "  time, description = random.choice(time_exp)\n",
    "  return time, description\n",
    "\n",
    "# look at some examples:\n",
    "for _ in range(5):\n",
    "  print(make_find_restaurant_expression())\n",
    "for _ in range(5):\n",
    "  print(make_make_booking_expression())\n",
    "for _ in range(5):\n",
    "  print(make_sort_resturants_expression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpakXkfBIZ-K"
   },
   "source": [
    "This programmatic approach immediately highlights a key advantage: **total control over data coverage and distribution**, so you're not limited to what people *currently* ask for, which often creates blind spots.\n",
    "\n",
    "For example, less common cuisines might rarely appear, preventing the model from ever learning them. By generating the structured commands first, we can systematically ensure that every parameter, function, and complex composition is well-represented in our training set.\n",
    "\n",
    "Of course, the descriptions we generated (`find restaurants that are Indian with a rating above 4`) are stilted and robotic. This brings us to the final, crucial step: using an LLM to paraphrase these canonical descriptions into a diverse set of natural-sounding user queries. Fortunately, this task plays directly to the primary strength of modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5DLeJnPWTiD",
    "outputId": "10e8cd07-d806-4838-8c7c-305cdb8893ab"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vllm import LLM\n",
    "# We can use a smaller, non-reasoning model. You still need a decent GPU, or you need to use a cloud model.\n",
    "# This will print a lot of initialization information when you first run the cell.\n",
    "# Running this cell multiple times may make VLLM reserve too much GPU memory, which will lead to errors.\n",
    "llm = LLM(model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "8ec178ddf303420186563191373bf5d1",
      "8a16e8d95d234c6fabb1cbb8ecb472db",
      "957e0f0c232e4b6fa7f8e45a6e280fd6",
      "8238c452076d4992abecb84e17ec5482",
      "3d2fef56600349dda57edb4e664a8aab",
      "32c9fd03263340b385d661f59bf36265",
      "86a3e92bb0ea403e8410caa3394e3193",
      "d5b33aba9d7a49bd946c738ff3da6836",
      "fb6558c4bf1c440dbe464d262146a020",
      "fdc90a181bcf452aa902b7a80d527131",
      "b7e24f96ad7f4939aa2fba75858a73ac",
      "a7072398f6b340bf821e3ce7eb5ecbeb",
      "609446a1ed2c4682a572163aac8a6a4f",
      "ac6cd4af08a74a8c9a21282608a3e2da",
      "e3f337ff51d040948dc6de452fd5b400",
      "ba8bb8a6924346f9b5d372add0a88d10",
      "56388b698cb042efb02fa726f2260bdd",
      "e92c6a71a7c243bea3ef7cd3c1045078",
      "cdd33b23e9304a43920f1dc561e5f933",
      "e9ce4cf6cf7c4dccb8100596f997fd61",
      "958fef65df0f4e33b1be00764d575828",
      "19f6ba00bca74e93b82aa20749aa5bca"
     ]
    },
    "id": "bf5aa869",
    "outputId": "4c3b5aad-4a54-4d33-a415-43651ae7941b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 195.16it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s, est. speed input: 2174.53 toks/s, output: 167.24 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 654.24it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.24it/s, est. speed input: 2784.08 toks/s, output: 195.23 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: find_restaurant(rating_gt=1)\n",
      "Description: find restaurants with a rating above  at least 1 rating\n",
      "Natural Questions: Show me restaurants with a rating above 1 ; Find restaurants with a rating above 1 ; Are there any restaurants rated higher than 1 stars?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "# Sampling parameters for text generation\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1000)\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that translates code and a stilted description into natural language requests for a restaurant booking API.\n",
    "You will be given pairs of code and a stilted description, and your task is to generate a semicolon-separated list of natural language requests that correspond to the given code and description.\n",
    "Only output the semicolon-separated list of natural language requests and nothing else. Here are some examples:\n",
    "Code: find_restaurant(cuisine=\"Italian\",rating_gt=2)\n",
    "Description: find restaurants that has cuisine Italian with a rating above  at least 2 rating\n",
    "Requests: Show me Italian restaurants with a rating over 2 ; Find Italian restaurants with a rating above 2 ; Are there any Italian places rated higher than 2 stars?\n",
    "\n",
    "Code: find_restaurant()\n",
    "Description: find restaurants\n",
    "Requests: Show me all restaurants ; Find restaurants ; List all restaurants\n",
    "\n",
    "Code: sort_restaurants(find_restaurant(cuisine=\"Indian\", rating_gt=3))[:3]\n",
    "Description: the best 3 restaurants that has cuisine Indian with a rating above  at least 3 rating\n",
    "Requests: What are the best 3 Indian restaurants with a rating above 3?; Show me the 3 best Indian restaurants with at least a 3 rating\n",
    "\n",
    "Code: make_booking(restaurant=find_restaurant(cuisine=\"Mexican\",), time=datetime.now() + timedelta(days=1))\n",
    "Description: make a booking for tomorrow at find restaurants that has cuisine Mexican\n",
    "Requests: Please make a booking for tomorrow at a Mexican restaurant; I'd like to make a reservation for tomorrow at a Mexican restaurant\n",
    "\n",
    "Now, turn the following code and description into natural language requests:\n",
    "Code: {code}\n",
    "Description: {description}\n",
    "Requests:\"\"\"\n",
    "\n",
    "def naturalize_expression(codes, descriptions):\n",
    "    \"\"\"Uses the LLM to turn the code and description into a natural language question using a chat template.\"\"\"\n",
    "    prompts = []\n",
    "    for code, description in zip(codes, descriptions):\n",
    "        message = [\n",
    "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(description=description, code=code)}\n",
    "        ]\n",
    "        prompt = llm.get_tokenizer().apply_chat_template(\n",
    "            message, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "        \n",
    "    batch_outputs = llm.generate(prompts, sampling_params)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    # Assuming the first output is the most relevant\n",
    "    return [output.outputs[0].text.strip() for output in outputs]\n",
    "\n",
    "# Test with an example, you can rerun this cell to see different examples.\n",
    "# VLLM is extremely fast at processing large batches of requests simultaneously.\n",
    "# To build a real dataset, you'd want to generate hundreds of prompts and send them all in a single batch.\n",
    "code, description = make_find_restaurant_expression()\n",
    "natural_question = naturalize_expression([code], [description])\n",
    "print(f\"Code: {code}\")\n",
    "print(f\"Description: {description}\")\n",
    "print(f\"Natural Questions: {natural_question[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xtrR-4NatP4"
   },
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Now you have a complete, scalable pipeline for generating high-quality training data for a semantic parser. By starting with the structured code and synthesizing natural language, you gain precise control over data coverage and can create a vast dataset with minimal manual effort. Note that the task is so simple that a relatively small LLM can be used to run it.\n",
    "\n",
    "The `(natural_question, code_string)` pairs can be used to:\n",
    "\n",
    "1.  **Train a specialized T5 model**: the most direct next step is to fine-tune an encoder-decoder model like [T5](https://huggingface.co/docs/transformers/en/model_doc/t5) or BART. This will result in a small, fast, and highly accurate parser perfect for a production environment.\n",
    "\n",
    "2.  **Fine-tune a larger LLM**: The same data can be used to fine-tune a more powerful LLM. This would make the model an expert at your specific API, capable of handling even more complex nuances.\n",
    "\n",
    "3.  **Create few-shot examples**: For the largest models, these high-quality pairs are perfect for creating few-shot prompts to guide the model's behavior at inference time without any fine-tuning at all.\n",
    "\n",
    "This \"generate-first\" methodology gives you a powerful toolkit for building robust natural language interfaces for any structured API."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19f6ba00bca74e93b82aa20749aa5bca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32c9fd03263340b385d661f59bf36265": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d2fef56600349dda57edb4e664a8aab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56388b698cb042efb02fa726f2260bdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "609446a1ed2c4682a572163aac8a6a4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56388b698cb042efb02fa726f2260bdd",
      "placeholder": "​",
      "style": "IPY_MODEL_e92c6a71a7c243bea3ef7cd3c1045078",
      "value": "Processed prompts: 100%"
     }
    },
    "8238c452076d4992abecb84e17ec5482": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdc90a181bcf452aa902b7a80d527131",
      "placeholder": "​",
      "style": "IPY_MODEL_b7e24f96ad7f4939aa2fba75858a73ac",
      "value": " 1/1 [00:00&lt;00:00, 98.29it/s]"
     }
    },
    "86a3e92bb0ea403e8410caa3394e3193": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a16e8d95d234c6fabb1cbb8ecb472db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32c9fd03263340b385d661f59bf36265",
      "placeholder": "​",
      "style": "IPY_MODEL_86a3e92bb0ea403e8410caa3394e3193",
      "value": "Adding requests: 100%"
     }
    },
    "8ec178ddf303420186563191373bf5d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a16e8d95d234c6fabb1cbb8ecb472db",
       "IPY_MODEL_957e0f0c232e4b6fa7f8e45a6e280fd6",
       "IPY_MODEL_8238c452076d4992abecb84e17ec5482"
      ],
      "layout": "IPY_MODEL_3d2fef56600349dda57edb4e664a8aab"
     }
    },
    "957e0f0c232e4b6fa7f8e45a6e280fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5b33aba9d7a49bd946c738ff3da6836",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb6558c4bf1c440dbe464d262146a020",
      "value": 1
     }
    },
    "958fef65df0f4e33b1be00764d575828": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7072398f6b340bf821e3ce7eb5ecbeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_609446a1ed2c4682a572163aac8a6a4f",
       "IPY_MODEL_ac6cd4af08a74a8c9a21282608a3e2da",
       "IPY_MODEL_e3f337ff51d040948dc6de452fd5b400"
      ],
      "layout": "IPY_MODEL_ba8bb8a6924346f9b5d372add0a88d10"
     }
    },
    "ac6cd4af08a74a8c9a21282608a3e2da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdd33b23e9304a43920f1dc561e5f933",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9ce4cf6cf7c4dccb8100596f997fd61",
      "value": 1
     }
    },
    "b7e24f96ad7f4939aa2fba75858a73ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba8bb8a6924346f9b5d372add0a88d10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "cdd33b23e9304a43920f1dc561e5f933": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5b33aba9d7a49bd946c738ff3da6836": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3f337ff51d040948dc6de452fd5b400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_958fef65df0f4e33b1be00764d575828",
      "placeholder": "​",
      "style": "IPY_MODEL_19f6ba00bca74e93b82aa20749aa5bca",
      "value": " 1/1 [00:00&lt;00:00,  2.63it/s, est. speed input: 1115.85 toks/s, output: 81.58 toks/s]"
     }
    },
    "e92c6a71a7c243bea3ef7cd3c1045078": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9ce4cf6cf7c4dccb8100596f997fd61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb6558c4bf1c440dbe464d262146a020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdc90a181bcf452aa902b7a80d527131": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
